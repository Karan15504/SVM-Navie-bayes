{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### **Question 1:** What is Information Gain, and how is it used in Decision Trees?\n",
        "\n",
        "**Answer:**  \n",
        "**Information Gain (IG)** measures how much uncertainty (entropy) in the target variable is reduced after splitting the data based on a particular feature.  \n",
        "It helps Decision Trees decide **which attribute to split on at each node**.\n",
        "\n",
        "Mathematically,  \n",
        "\\[\n",
        "IG(T, X) = Entropy(T) - \\sum_{v \\in Values(X)} \\frac{|T_v|}{|T|} \\times Entropy(T_v)\n",
        "\\]\n",
        "\n",
        "Where:  \n",
        "- **T:** The entire dataset  \n",
        "- **X:** The attribute being tested  \n",
        "- **Tᵥ:** The subset of data where X has value v  \n",
        "\n",
        "A feature with higher Information Gain is more effective in classifying the data correctly.  \n",
        "Hence, the algorithm picks the attribute with the **highest Information Gain** for splitting at each step.\n"
      ],
      "metadata": {
        "id": "kt3n1BgZCJSS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Question 2:** What is the difference between Gini Impurity and Entropy?\n",
        "\n",
        "**Answer:**  \n",
        "Both **Gini Impurity** and **Entropy** are measures of impurity used in Decision Trees to evaluate the quality of splits.\n",
        "\n",
        "| Criterion | Formula | Range | Best (Pure) Value | Interpretation |\n",
        "|------------|----------|--------|------------------|----------------|\n",
        "| **Entropy** | \\(-\\sum p_i \\log_2(p_i)\\) | 0 to 1 | 0 | Measures disorder or randomness in data. |\n",
        "| **Gini Impurity** | \\(1 - \\sum p_i^2\\) | 0 to 0.5 (for binary) | 0 | Measures the probability of incorrect classification. |\n",
        "\n",
        "**Comparison:**  \n",
        "- **Entropy** uses logarithms and gives more weight to rare classes.  \n",
        "- **Gini** is computationally faster since it avoids log calculations.  \n",
        "- Both usually produce similar results, but **Gini** is preferred for speed and **Entropy** for interpretability.\n"
      ],
      "metadata": {
        "id": "F1Dfa6dPCLWq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Question 3:** What is Pre-Pruning in Decision Trees?\n",
        "\n",
        "**Answer:**  \n",
        "**Pre-pruning** (also known as **early stopping**) is a technique used to **stop the tree from growing too deep**.  \n",
        "Instead of allowing the tree to fully grow and then trimming it (post-pruning), pre-pruning applies restrictions during training.\n",
        "\n",
        "Common pre-pruning conditions include:  \n",
        "- Maximum depth of the tree (`max_depth`)  \n",
        "- Minimum number of samples to split (`min_samples_split`)  \n",
        "- Minimum leaf node size (`min_samples_leaf`)  \n",
        "- Minimum information gain threshold  \n",
        "\n",
        "This method helps prevent **overfitting**, reduces computational cost, and improves model generalization.\n"
      ],
      "metadata": {
        "id": "ZfFDu4heCau8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Question 4:** Write a Python program to train a Decision Tree Classifier using Gini Impurity as the criterion and print the feature importances.\n"
      ],
      "metadata": {
        "id": "8Y1jJYuaCdap"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "iris = load_iris()\n",
        "X = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
        "y = iris.target\n",
        "\n",
        "\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "clf.fit(X, y)\n",
        "\n",
        "\n",
        "for feature, importance in zip(X.columns, clf.feature_importances_):\n",
        "    print(f\"{feature}: {importance:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v02RxGdCCiUY",
        "outputId": "99435d7d-a2fe-46c1-9bab-337974b72cbb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sepal length (cm): 0.0133\n",
            "sepal width (cm): 0.0000\n",
            "petal length (cm): 0.5641\n",
            "petal width (cm): 0.4226\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Question 5:** What is a Support Vector Machine (SVM)?\n",
        "\n",
        "**Answer:**  \n",
        "A **Support Vector Machine (SVM)** is a supervised learning algorithm used for both **classification and regression** tasks.  \n",
        "It works by finding the **best hyperplane** that separates data points of different classes with the **maximum margin**.  \n",
        "\n",
        "The data points that are closest to this boundary are called **support vectors** — they are critical for defining the decision boundary.\n",
        "\n",
        "**Key Advantages:**  \n",
        "- Works well in high-dimensional spaces.  \n",
        "- Effective even when the number of features exceeds the number of samples.  \n",
        "- Uses different kernel functions to handle non-linear data.\n"
      ],
      "metadata": {
        "id": "lMyx1EFmCpXO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Question 6:** What is the Kernel Trick in SVM?\n",
        "\n",
        "**Answer:**  \n",
        "The **Kernel Trick** allows SVMs to handle **non-linear relationships** without explicitly transforming data into higher dimensions.  \n",
        "\n",
        "A **kernel function** computes similarity between data points in an implicit high-dimensional feature space.  \n",
        "Common kernel types include:  \n",
        "- **Linear Kernel:** Used for linearly separable data.  \n",
        "- **Polynomial Kernel:** Maps data into polynomial feature space.  \n",
        "- **RBF (Radial Basis Function) Kernel:** Handles complex, non-linear boundaries.  \n",
        "\n",
        "In short, the kernel trick enables SVMs to **separate complex datasets efficiently** without ever computing the higher-dimensional transformation directly.\n"
      ],
      "metadata": {
        "id": "IDksxgYfCuFW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Question 7:** Write a Python program to train two SVM classifiers with Linear and RBF kernels on the Wine dataset, then compare their accuracies.\n"
      ],
      "metadata": {
        "id": "MAT199-PCwsu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "wine = load_wine()\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    wine.data, wine.target, test_size=0.3, random_state=42)\n",
        "\n",
        "svm_linear = SVC(kernel='linear')\n",
        "svm_linear.fit(X_train, y_train)\n",
        "linear_acc = accuracy_score(y_test, svm_linear.predict(X_test))\n",
        "\n",
        "\n",
        "svm_rbf = SVC(kernel='rbf')\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "rbf_acc = accuracy_score(y_test, svm_rbf.predict(X_test))\n",
        "\n",
        "print(\"Linear Kernel Accuracy:\", round(linear_acc, 3))\n",
        "print(\"RBF Kernel Accuracy:\", round(rbf_acc, 3))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nqqOitqeCzNY",
        "outputId": "44d083a0-67f0-4f5e-c98f-dc31af621ac7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear Kernel Accuracy: 0.981\n",
            "RBF Kernel Accuracy: 0.759\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Question 8:** What is the Naïve Bayes classifier, and why is it called \"Naïve\"?\n",
        "\n",
        "**Answer:**  \n",
        "The **Naïve Bayes classifier** is a probabilistic model based on **Bayes’ Theorem**, which predicts class membership probabilities based on prior knowledge and observed data.\n",
        "\n",
        "It is called “Naïve” because it **assumes all features are independent** of each other — an assumption rarely true in real-world data, yet surprisingly effective.\n",
        "\n",
        "Formula:  \n",
        "\\[\n",
        "P(C|X) = \\frac{P(X|C) \\times P(C)}{P(X)}\n",
        "\\]  \n",
        "Where:  \n",
        "- \\(P(C|X)\\): Posterior probability of class C given data X  \n",
        "- \\(P(X|C)\\): Likelihood of data given class  \n",
        "- \\(P(C)\\): Prior probability of the class  \n",
        "\n",
        "Despite its simplicity, Naïve Bayes works well in text classification, spam filtering, and sentiment analysis.\n"
      ],
      "metadata": {
        "id": "U533-X6JC5ba"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Question 9:** Explain the differences between Gaussian Naïve Bayes, Multinomial Naïve Bayes, and Bernoulli Naïve Bayes.\n",
        "\n",
        "**Answer:**  \n",
        "| Type | Suitable For | Key Idea | Example Use Case |\n",
        "|------|---------------|-----------|------------------|\n",
        "| **Gaussian NB** | Continuous data | Assumes data follows a normal (Gaussian) distribution | Predicting diseases using continuous lab results |\n",
        "| **Multinomial NB** | Count-based data | Works with discrete features like word counts | Text classification, spam filtering |\n",
        "| **Bernoulli NB** | Binary features | Features are either 0 or 1 (present/absent) | Sentiment analysis or document classification |\n",
        "\n",
        "Each variant is optimized for different data types but follows the same core principle of Bayes’ theorem.\n"
      ],
      "metadata": {
        "id": "GlJw-wzmC8pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Question 10:** Write a Python program to train a Gaussian Naïve Bayes classifier on the Breast Cancer dataset and evaluate accuracy.\n"
      ],
      "metadata": {
        "id": "YLXxvqt1DALw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    data.data, data.target, test_size=0.3, random_state=42)\n",
        "\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "y_pred = gnb.predict(X_test)\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy of Gaussian Naive Bayes:\", round(acc, 3))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V8gJd--KDDuO",
        "outputId": "a9fdde7d-2e6a-4c1e-dcbc-489f4220c1f0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Gaussian Naive Bayes: 0.942\n"
          ]
        }
      ]
    }
  ]
}